---
title: "Multiple Linear Regression"
author: "Addison van den Hoeven"
date: "July 17, 2018"
output: 
  prettydoc::html_pretty:
  theme: cayman
  highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)
```

# Multiple Linear Regression (MLR)

In many applications, there is more than one variable that influences the response. Multiple Regression models describe how a single response variable $Y$ depends on a number of predictor variables.

When this model is linear in the coefficients, it is called a **Multiple Linear Regression Model**. The MLR takes the form:

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X2 + ... + \beta_k X_k + \epsilon$$

If we set $X_0 = 1$ then we may use the compact formula:

$$Y = \sum^k_{j=1} \beta_j X_j + \epsilon$$
or the vector form:

$$Y = X^T \beta + \epsilon$$

Our goal is to seek estimators of the parameters $\beta_0, ..., \beta_k$ by fitting the Multiple Linear Regression model to the data.

$$\{ (x_{1i}, x_{2i},...,x_{ki} ;y_i ); i=1,2,...,n \quad \text{and} \quad n>k \}$$
where $y_i$ is the observed response to the values $x_{1i},x_{2i},...,x_{ki}$ of the $k$ independent variables $x_1,x_2,...,x_k$.

In matrix form

$$Y=\boldsymbol{X} \beta + \epsilon$$
where

$$
Y=
\begin{bmatrix}
    y_1 \\
    y_2 \\
    ...\\
    y_n 
\end{bmatrix}
,
X=
\begin{bmatrix}
    1 & x_{11}       & x_{12} & x_{31} & \dots & x_{k1} \\
    1 & x_{12}       & x_{22} & x_{32} & \dots & x_{k2} \\
    ... & ... & ... & ... & ... & ...\\
    1 & x_{1n}       & x_{2n} & x_{3n} & \dots & x_{kn}
\end{bmatrix}
,
\beta=
\begin{bmatrix}
    \beta_0 \\
    \beta_1 \\
    ...\\
    \beta_k 
\end{bmatrix}
,
\epsilon=
\begin{bmatrix}
    \epsilon_1 \\
    \epsilon_2 \\
    ...\\
    \epsilon_n 
\end{bmatrix}
$$

The Least Squares method for estimation of $\beta$ involves finding $\hat{\beta}$ for which

$$RSS = E(\beta) = (\boldsymbol{X} \beta - y)^T (\boldsymbol{X} \beta - y)$$
is minimized. Note that $(\boldsymbol{X} \beta - y)^T (\boldsymbol{X} \beta - y)$ is essentially the same as when we did $(y_i - \hat{y_i})^2$ in simple linear regression. 

We find the gradient of the $RSS$:

$$
\begin{aligned}
\nabla_{\beta} RSS &= \nabla ((\boldsymbol{X} \beta - y)^T (\boldsymbol{X} \beta - y)) \\
\nabla_{\beta} RSS &= \nabla (\beta^T \boldsymbol{X}^T \boldsymbol{X} \beta - y^T \boldsymbol{X} \beta - \beta^T \boldsymbol{X} ^T y + y^T y) \\
\nabla_{\beta} RSS &= 2 \boldsymbol{X}^T \boldsymbol{X} \beta - 2 \boldsymbol{X}^T y\\
\end{aligned}
$$

Setting the gradient equal to zero we get:

$$
\begin{aligned}
2 \boldsymbol{X}^T \boldsymbol{X} \beta - 2\boldsymbol{X}^T y = 0 &\Leftrightarrow \boldsymbol{X}^T\boldsymbol{X}\beta = \boldsymbol{X}^T y \\
& \Leftrightarrow \hat{\beta} = (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T y
\end{aligned}
$$
if $A=\boldsymbol{X}^T \boldsymbol{X}$ is invertible. Recall that a matrix A is invertible if there exists an $n$ by $n$ square matrix $B$ such that:
$$AB = BA = I_n$$
where $I_n$ denotes the $n$ x $n$ identity matrix. 


## Examples using the "House Prices: Advanced Regression Techniques" dataset from kaggle.com

Since the math is a little more complex than in simple linear regression, we will use the intuition learned from simple linear regression and get right into practical examples where we will look at theory along the way. 

```{r}
df <- read.csv("house_train.csv")

names(df)

```

As you can see above we've got a lot of features. And if you were to look at the dataset many of the features take different forms. Some are quantitative in nature such as lot size, others are qualitative in nature like the type of foundation, and some are binary like whether or not there is central heating (we will call this a dummy variable). 

```{r}
plot(x = df$Id, 
     y = df$SalePrice,
     xlab = "Id",
     ylab = "Sale Price",
     main = "Sale Price of 1460 homes",
     col="blue")
```

If we try and do a Simple Linear Regression model using only the Lot Area, we get the following. 

```{r}
plot(x = df$LotArea, 
     y = df$SalePrice,
     xlab = "LotArea",
     ylab = "Sale Price",
     main = "Sale Price of 1460 homes",
     col="blue")

linear_model <- lm(SalePrice ~ LotArea, data=df)

abline(linear_model, col="tomato")
```

We can see a pretty weak relationship between the two. This makes a lot of sense since we are neglecting the other 70 or so pieces of information we have about each home.

```{r}


#linear_model <- lm(SalePrice ~ , data=df)

```

### Feature Manipulation