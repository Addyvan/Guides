---
title: "Simple Linear Regression"
author: "Addison van den Hoeven"
date: "July 3, 2018"
output: 
  prettydoc::html_pretty:
  theme: cayman
  highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview of Linear Regression

Linear Regression is a very simple approach for performing supervised learning. It is a useful tool for predicting a quantitative response variable $Y$ (also called the dependent variable) using input variables $X_1,...,X_n$ (also called independent variables or regressors) using a simple linear model.

Although it may seem overly simplistic, linear regression is extremely useful both conceptually and practically.

* It is still a widely used statistical learning model
* It serves as a great starting point for newer approaches: many fancy statistical learning approaches can be seen as generalizations or extensions of linear regression

## Intuition behind using models

Suppose that your role is to develope a marketing plan, on the basis of data, for next year that will result in high product sales.

Questions we might ask would be:

* Is there a relationship between advertising budget and sales?
* How strong is the relationship?
* Which media contribute to sales?
* How can we estimate the the effect of each medium on sales?
* How accurately can we predict future sales?
* Is the relationship linear? 

Linear Regression can be used to answer all these questions.

## Simple Linear Regression

Simple Linear (SL) Regression is a simple approach for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes there is approximately a linear relationship between $X$ and $Y$. 

Mathematically, we can write this linear relationship as:

$$Y \approx \beta_0 + \beta_1 X$$
where $\approx$ denotes "approximately modeled as".

In the above equation $\beta_0$ and $\beta_1$ are two unknown constants that denote the intercept and slope of the linear model. They are generally referred to as model coefficients or model parameters. Our goal is to find these values.

For example, $X$ may represent visits to the TBS Open Data Portal and $Y$ may represent the number of data downloads from the portal. Below is an example of linear regression used on real data disclosed by TBS on the open data portal:

```{r, echo = FALSE}
df <- read.csv("TBSOpenDataVisits.csv", header = TRUE)


plot(x = df$visits, 
     y = df$downloads,
     xlab = "Visits",
     ylab = "Data Downloads",
     main = "TBS Open Data Portal Downloads ~ Visits",
     col="blue")

linear_model <- lm(downloads ~ visits, data=df)

abline(linear_model, col="tomato")

```

It is important to remark that:

* In most scientific / engineering phenomena, the relationship between $X$ and $Y$ is not necessarily linear
* It is also not necessarily deterministic in the sense that a given value $X$ does not always give the same value for $Y$

As a result, these problems are probabilitisc in nature since the relationship is not exact. The concept of regression analysis deals with finding the best relationship between $Y$ and $X$.

Because of this we assume a model where we include an error term:

$$Y = \beta_0 + \beta_1 X + \epsilon$$ where $\epsilon$ is the error term

Given some estimates $\hat{\beta_0}$ and $\hat{\beta_1}$ for the model coefficients, we predict downloads using

$$ \hat{y} = \hat{\beta_0} + \hat{\beta_1} x $$ where $\hat{y}$ indicates a prediction of $Y$ on the basis of $X=x$.

* The hat symbol denotes an estimated value.

In practice, $\beta_0$ and $\beta_1$ are unknown. We must use the data to estimate the coefficients/parameters. Now we get into the nitty gritty of regression and actually do a little math to see how we can turn raw data into an approximate linear model.

## Math behind Simple Linear Regression

If you have never derived any statistical or learning model before, simple linear regression is a great place to start. The principles and intuition behind SLR will be invaluable when working towards more complex problems. 

Before diving in, just know that it is assumed that you have some familiarity with summation notation, basic calculus, and some sense of statistics. If this is not the case I would still recommend diving in as there is a good chance things will clear up as you read forward and if that is not the case your eyes will have picked up a few things that will trigger a response when you do go to do a little catchup in these domains. Additionaly, moving forward, an understanding of linear algebra and matrix calculus will be very helpful. When working in this space it never hurts to understand more of the math then needed.

Speaking from experience, it may also be helpful to grab some paper and pen/pencil and write down the equations yourself. Solving things on your own is a surefire way to keep things in memory and it may also lead to the creation of your own personalized notes on the subject. Lastly, there is no need to memorize every piece of a derivation. It is there to help you understand and often you do not need to hold the derivatives in memory to fully understand a model's use case and potential.

### Derivation of Simple Linear Regression: least squares approach

Let $(x_1, y_1), (x_2, y_2),...,(x_n, y_n)$ represent $n$ observation pairs, each of which consists of a measurement of $X$ and $Y$. These value pairs are called the training data. 

Let $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_i$ be the prediction for $Y$ based on the $i$th value of $X$.

Then $e_i = y_i - \hat{y_i}$ represents the $i$th residual.

We define the Sum of Squared Errors (SSE) also called the Residual Sum of Squares (RSS) as 

$$E(\beta_0, \beta_1) = RSS = e^2_1 + e^2_2 + ... + e^2_n = \sum^n_{i=1} (y_i - \hat{\beta_0} - \hat{\beta_1} x_i)^2$$
Our goal is to minimize the RSS, effectively minimizing the amount our model is incorrect. To do this we calculate the partial derivatives of $\beta_0$ and $\beta_1$ and set them to zero.

#### Finding the partial derivative of $RSS$ with respect to $\beta_0$

The partial derivative w.r.t. (with respect to) $\beta_0$:

$$ \frac{\partial E}{\partial \beta_0}  = \frac{\partial}{\partial \beta_0} \Big[ \sum^n_{i=1} (\beta_0 + \beta_1 x_i - y_i)^2 \Big] = 0$$
Utilizing [chain rule](https://www.math.ucdavis.edu/~kouba/CalcOneDIRECTORY/chainruledirectory/ChainRule.html): 
$$ \Leftrightarrow 2 \sum^n_{i=1}(\beta_0 + \beta_1 x_i - y_i) \frac{\partial}{\partial \beta_0} [\beta_0 + \beta_0 + \beta_1 x_i - y_i] = 0$$
$$ \Leftrightarrow \sum^n_{i=1} (\beta_0 + \beta_1 x_i - y_i) (1 + 0 - 0) = \sum^n_{i=1} (\beta_0 + \beta_1 x_i - y_i) = 0$$
Utilizing [Summation Properties](http://tutorial.math.lamar.edu/Classes/CalcI/SummationNotation.aspx):
$$ \Leftrightarrow n \beta_0 + \beta_1 \sum^n_{i=1} x_i - \sum^n_{i=1} y_i = 0$$
$$ \Leftrightarrow n \beta_0 + \beta_1 \sum^n_{i=1} x_i = \sum^n_{i=1} y_i$$

#### Finding the partial derivative of $RSS$ with respect to $\beta_1$

The partial derivative w.r.t $\beta_1$:

$$ 
\begin{aligned} 
\frac{\partial E}{\partial \beta_1} &= \frac{\partial}{\partial \beta_1} \Big[ \sum^n_{i=1} (\beta_0 + \beta_1 x_i - y_i)^2 \Big] = 0 \\
&\Leftrightarrow 2 \sum^n_{i=1}(\beta_0 + \beta_1 x_i - y_i) \frac{\partial}{\partial \beta_1} [\beta_0 + \beta_0 + \beta_1 x_i - y_i] = 0 \\
&\Leftrightarrow \sum^n_{i=1}(\beta_0 + \beta_1 x_i - y_i)x_i = 0 \\
&\Leftrightarrow \beta_0 \sum^n_{i=1} x_i + \beta_1 \sum^n_{i=1} x_i^2 - \sum^n_{i=1} x_i y_i = 0 \\ &\Leftrightarrow \beta_0 \sum^n_{i=1} x_i + \beta_1 \sum^n_{i=1} x_i^2 = \sum^n_{i=1} x_i y_i \end{aligned}
 $$

#### Finding $\beta_0$ and $\beta_1$ by minimizing $RSS$

Now that we have found both partial derivatives, we can use them to solve for $\beta_0$ and $\beta_1$ by setting both partial derivatives equal to 0:

$$
\begin{aligned}
\frac{\partial E}{\partial \beta_0} &= n \beta_0 + \beta_1 \sum^n_{i=1} x_i - \sum^n_{i=1} y_i = 0\\
\frac{\partial E}{\partial \beta_1} &= \beta_0 \sum^n_{i=1} x_i + \beta_1 \sum^n_{i=1} x_i^2 - \sum^n_{i=1} x_i y_i = 0
\end{aligned}
$$
$$ n \beta_0 + \beta_1 \sum^n_{i=1} x_i - \sum^n_{i=1} y_i = \beta_0 \sum^n_{i=1} x_i + \beta_1 \sum^n_{i=1} x_i^2 - \sum^n_{i=1} x_i y_i$$
The slope term is:

$$
\begin{aligned}
\hat{\beta_1} &= \frac{n\sum^n_{i=1} x_i y_i - (\sum^n_{i=1} x_i)(\sum^n_{i=1} y_i)}{n \sum^n_{i=1} x_i^2 - (\sum^n_{i=1} x_i)^2} \\
&= \frac{\sum^n_{i=1} (x_i - \bar{x})(y_i - \bar{y})}{\sum^n_{i=1} (x_i - \bar{x})^2} \\
&= \frac{Cov(x,y)}{Var(x)}
\end{aligned}
$$
Where $\bar{y} = \frac{1}{n}\sum^n_{i=1} y_i$ and $\bar{x} = \frac{1}{n}\sum^n_{i=1} x_i$ are the sample means.

The intercept term is:

$$
\begin{aligned}
\hat{\beta_0} &= \frac{\sum^n_{i=1} y_i - \hat{\beta_1} \sum^n_{i=1} x_i}{n}
&= \bar{y} - \hat{\beta_1} \bar{x}
\end{aligned}
$$

## Least Squares

The above method of performing linear regression is referred to as Ordinary Least Squares Estimation (OLSE).

#### Why least squares?

The least squares method is designed to minimize the sum of the squares of the residuals and to provide a fitted line that results in a closeness between the line and the plotted points.

There are many ways of measuring closeness. For example, one may wish to determine $\hat{\beta_0}$ and $\hat{\beta_1}$ using another formula of the combined residuals such as:

$$\sum^n_{i=1} |y_i - \hat{y_i}|$$ or $$\sum^n_{i=1} |y_i - \hat{y_i}|^{\frac{3}{2}}$$

However, the least squares method has the advantage to produce a differentiable formula and fit with the known formula of the variance of a random variable.

#### Properties of least squares estimators

* In the equation $Y= \beta_0 + \beta_1 X + \epsilon$, the error term $\epsilon$ is assumed to be a normal random variable with $E(\epsilon) = 0$ and $Var(\epsilon) = \sigma^2$.
* Thus, $Y$ is also a normal random variable. In general $X$ is not random and can be measured with negligible error.
* The fact that $E(\epsilon) = 0$ implies that at a specific $X$, the $Y$ values are distributed around the true or population regression line $Y= \beta_0 + \beta_1 X$

![](SimpleLinearFig1.png)

* $Y$ is a random variable for each fixed value of $X$. Observe that:

$$E(Y|X) = \beta_0 + \beta_1 X$$

* The true regression line goes through the means of the response, and the actual observations are on the distribution around the means.

Suppose we make the assumption that $\epsilon_1,...,\epsilon_n$ are independent from run to run in the experiment. This provides a foundation for finding means and variances for the estimators $\beta_0$ and $\beta_1$.

We recall that $\hat{\beta_0}$ and $\hat{\beta_1}$ are only estimates of true parameters $\beta_0$ and $\beta_1$ based on a given sample of $n$ observations. If the experiment is repeated over and over again, each time using the same fixed values of $x$, the resulting estimates of $\beta_0$ and $\beta_1$ will most likely differ from experiment to experiment.

Thus, the estimates of $\beta_0$ and $\beta_1$ may be viewed as values assumed by the random variables $B_0$ and $B_1$ which can be considered as estimators for $\beta_0$ and $\beta_1$. Thus, $\hat{\beta_0}$ and $\hat{\beta_1}$ are specific realizations of the estimators. 

Since the values of $X$ remain fixed, the values of $\beta_0$ and $\beta_1$ depend on the variations in the values of $Y$ or, more precisely, on the values of the random variable $Y_1, Y_2, ...,Y_n$. The distributional assumptions imply that $Y_i, i=1,2,...,n$ are also independently distributed, with mean $E(Y_i) = E(Y|x_i) = \beta_0 + \beta_1 x_i$ and equal variances $\sigma^2$; that is:

$$ \sigma_{Y|x_i}^2 = \sigma^2 \quad \text{  for  } \quad i=1,2,...,n$$
#### The least squares estimators for $\beta_0$ and $\beta_1$

$$
\begin{aligned}
B_1 &= \frac{\sum^n_{i=1} (x_i - \bar{x})(Y_i - \bar{Y})}{\sum^n_{i=1} (x_i - \bar{x})^2} = \frac{\sum^n_{i=1} (x_i - \bar{x})Y_i}{\sum^n_{i=1} (x_i - \bar{x})^2} \\
&= \sum^n_{i=1} c_i Y_i
\end{aligned}
$$
where
$$
c_i = \frac{x_i - \bar{x}}{\sum^n_{i=1} (x_i - \bar{x})^2}, \quad i=1,...,n
$$

It follows that $B_1$ is a normal distribution with mean

$$
\mu_{B_1} = \frac{\sum^n_{i=1} (x_i - \bar{x})(\beta_0 + \beta_1 x_i)}{\sum^n_{i=1} (x_i - \bar{x})^2} = \beta_1
$$
hence, it is an unbiased estimator of $\beta_1$. Its variance is

$$
\sigma^2_{B_1} = \frac{\sum^n_{i=1} (x_i - \bar{x})^2 \sigma^2_{Y_i}}{\Big [  \sum^n_{i=1} (x_i - \bar{x})^2  \Big ]^2 } = \frac{\sigma^2}{\sum^n_{i=1} (x_i - \bar{x})^2}
$$

The least squares estimator for $\beta_0$ is given by

$$B_0 = \bar{Y} - \bar{x} B_1$$

It follows that $B_0$ is also normally distributed with mean

$$ \mu_{B_0} = E(\bar{Y}) - \bar{x} E(B_1) = \beta_0 $$
hence, it is an unbiased estimator of $\beta_0$.

Its variance is 

$$ \sigma^2_{B_0} = \frac{\sum^n_{i=1} x_i^2}{n \sum^n_{i=1} (x_i -\bar{x})^2} \sigma^2 $$

To draw inferences on $\beta_0$ and $\beta_1$, it becomes necessary to arrive at an estimate of the parameter $\sigma^2$ appearing in the two preceding variance formulas for $B_0$ and $B_1$.

The parameter $\sigma^2$, the model error variance, reflects random variation or experimental error variation around the regression line. We define:

$$
\begin{aligned}
S_{xx} &= \sum^n_{i=1} (x_i - \bar{x})^2 \\
S_{yy} &= \sum^n_{i=1} (y_i - \bar{y})^2\\
S_{xy} &= \sum^n_{i=1} (x_i - \bar{x})(y_i - \bar{y})
\end{aligned}
$$

Now we may write

$$
\begin{aligned}
RSS &= S_{yy} - 2 \hat{\beta_1} S_{xx} + \hat{\beta_1}^2 S_{xx}\\
&= S_{yy} - \hat{\beta_1} S_{xy} \quad (\text{since} \quad \hat{\beta_1} = S_{xy}/S_{xx})
\end{aligned}
$$

It follows that

$$
s^2 = \frac{RSS}{n-2} = \sum^n_{i=1} \frac{(y_i - \hat{y_i})^2}{n-2} = \frac{S_{yy} - \hat{\beta_1} S_{xy}}{n-2}
$$
is an unbiased estimator of $\sigma^2$.

The parameter $\sigma^2$ measures the squared deviations between $Y$ values and their mean given by $E(Y/x)$, i.e., squared deviations between $Y$ and $\beta_0 + \beta_1 x$ which is estimated by $\hat{y} = \hat{\beta_0} + \hat{\beta_1} x$.

Thus it would make sense that $\sigma^2$ is best depicted as a squared deviation of the typical observation $y_i$ from the estimated mean $\hat{y_i}$. 

The quantity $s^2$ is called a mean-squared error (MSE).

## Inferences about the Regression Coefficients

### Statistics of Linear Regression using Least Squares

Under the normality assumption, we can prove that the random variable 

$$\frac{(n-2)S^2}{\sigma^2}$$
where
$$S^2 = \frac{RSS}{n-2}$$
is a chi-squared variable with $n-2$ degrees of freedom and it is independent of the random variable $B_1$.

It follows that the statistic 

$$T = \frac{(B_1 - \beta_1)/(\sigma / \sqrt{S_{xx}})}{S/ \sigma} = \frac{B_1 - \beta_1}{S / \sqrt{S_{xx}}}$$
has a $t$-distribution with $n-2$ degrees of freedom.

The statistic $T$ can be used to construct a $100(1 - \alpha) \%$ confidence interval for the coefficient $\beta_1$.

A $100(1- \alpha)\%$ confidence interval for the parameter $\beta_1$ in the regression line

$$E(Y/x) = \beta_0 + \beta_1 x$$
is given by
$$\hat{\beta_1} - t_{\alpha / 2} \frac{s}{\sqrt{S_{xx}}} < \beta_1 < \hat{\beta_1} + t_{\alpha / 2} \frac{s}{\sqrt{S_{xx}}}$$
where $t_{\alpha / 2}$ is a value of the $t$-distribution with $v=n-2$ degrees of freedom leaving an area of $\alpha / 2$ to the right.

Standard errors can also be used to perform hypothesis tests on the coefficients. The most common hypothesis test involves testing the null hypothesis of:
$$H_0: \text{There is no relationship between X and Y}$$
or
$$H_1: \text{There is some relationship between X and Y}$$

Mathematically this corresponds to testing:
$$H_0: \beta_1 = 0$$
versus
$$H_1: \beta_1 \neq 0$$
since if $\beta_1 = 0$ then the model reduces to $Y= \beta_0 + \epsilon$ and $X$ is not associated with $Y$.

To test the null hypothesis, we compute a $t$-statistic, given by
$$t=\frac{\hat{\beta_1} - 0}{SE(\hat{\beta_1})} = \frac{\hat{\beta_1} - 0}{s/\sqrt{S_{xx}}}$$
This has a distribution with $n-2$ degrees of freedom. Using software it is easy to compute the probability of observing any value equal to $t$ or larger. This value is called the $p$ value. Roughly speaking we can interpret the p value as follows:

* Small $p$-value $\Rightarrow$ it is unlikely to observe such a substancial association betweeb the predictor and the response due to chance, in the absence of any real association between the predictor and the response. 

Confidence intervals and hypothesis testing on the coefficient $\beta_0$ may be established from the fact that $B_0$ is also normally distributed.

It is not difficult to show that
$$T= \frac{B_0 - \beta_0}{S \sqrt{\sum^n_{i=1} x_i^2 /(nS_{xx})}}$$
has a $t$-distribution with $n-2$ degrees of freedom.

We can now construct a $100(1-\alpha)\%$ confidence interval for the coefficient $\beta_0$.

### Quality of Fit

To assess the quality of fit, i.e., how well the simple linear regression model fits the data, we use the Coefficient of Determination, also known as $R$-squared ($R^2$). 

We can defined the coefficient of determination as the proportion of the variation in the target variable $y$ explained by the linear regression. It is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable. 

Lets define the following quantities:

* Total Sum of Squares: $TSS=\sum^n_{i=1} (y_i - \bar{y})^2$
* Explained Sum of Squares: $ESS = \sum^n_{i=1} (\hat{y_i} - \bar{y})^2$

So we see that $TSS$ is essentially a measure of the total variation in $y$ explained by both the regression model and the residual $\epsilon$. Note that $\bar{\hat{y}} = \bar{y}$:

$$
\begin{aligned}
\bar{\hat{y}} &= \frac{1}{n} \sum^n_{i=1} \hat{y_i} = \frac{1}{n} \sum^n_{i=1} \hat{\beta_0} + \hat{\beta_1} x_i\\
&= \hat{\beta_0} + \hat{\beta_1}\bar{x} = \bar{y}- \hat{\beta_1} \bar{x} + \hat{\beta_1} \bar{x}\\
&= \bar{y}
\end{aligned}
$$
It follows that
$$ESS = \sum^n_{i=1} (\hat{y_i} - \bar{y})^2 = \sum^n_{i=1} (\hat{y_i} - \bar{\hat{y}})^2$$
is the variation in the predicted data. 

In other words, it measures the variation explained by the fitted model.

Now, we recall that 
$$RSS = \sum^n_{i=1} (y_i - \hat{y_i})^2$$
which is the variation due to error, and usually called the unexplained variation. 

The following equality, stating that the total sum of squares equals the residual sum of squares plus the explained sum of squares, is generally true in simple linear regression:

$$
\begin{aligned}
\sum^n_{i=1} (y_i - \bar{y})^2 &= \sum^n_{i=1} (y_i - \hat{y_i})^2 + \sum^n_{i=1} (\hat{y_i} - \bar{y})^2 \\
TSS &= RSS + ESS
\end{aligned}
$$

The coefficient of determination $R^2$ is defined as 
$$R^2 = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS}$$

$0 \leq R^2 \leq 1$ is the proportion of total variation in the target variable explained by the regression model. 

If $RSS = 0 \Rightarrow $, all residuals are zero $\Rightarrow$ the fit is perfect. Hence, in this case all the variation is explained and $R^2 = 1$. 

On the other hand, for a poor fitting model, $RSS$ is large (close to $TSS$) and $ESS$ is small. And consequently, $R^2$ will be small (close to 0).

The problem with the coefficient of determination $R^2$ is that in a given application we don't necessarily know what acceptable values of $R^2$ we may use to assess the model.

* Adding additional terms to the model (e.g. an additional regressor) decreases $RSS$ and thus increases $R^2$.

* This implies that $R^2$ can be made artificially high by an unwise practice of overfitting (i.e. the inclusion of too many model terms).

## Prediction

One of the main objectives of building a linear regression model is to predict response values at one or more values of the independent variable. 

The equation 
$$\hat{y} = \hat{\beta_0} + \hat{\beta_1} x$$
may be used to predict or estimate the mean response $\mu_{Y|x_0}$ at $x=x_0$, where $x_0$ is not necessarily one of the pre-chosen values in a the training data. It may also be used to predict a single value $y_0$ of the variable $Y_0$, when $x=x_0$.

Suppose that the experimenter wishes to construct a confidence interval for $\mu_{Y|x_0}$. We may use the estimator $\hat{Y_0} = B_0 + B_1 x_0$ to estimate
$$\mu_{Y| x_0} = \beta_0 + \beta_1 x_0$$
It can be shown that the sampling distribution of $\hat{Y_0}$ is normal with mean
$$E(\hat{Y_0}) = \mu_{Y|x_0}$$
Hence, it is an unbiased estimator of $\mu_{Y|x_0}$.

The variance of $\hat{Y_0}$ is given by
$$
\begin{aligned}
\sigma^2_{\hat{Y_0}} &= \sigma_{B_0 + B_1 x_0}^2 = \sigma^2_{\bar{Y} + B_1 (x_0 - \bar{x})} \\
&= \sigma^2 \Big [ \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{S_{xx}}  \Big]
\end{aligned}
$$
Note that in order to prove the above equation one needs to prove that 
$$Cov(\bar{Y}, B_1) = 0$$

Thus, a $100(1-\alpha)\%$ confidence on the mean response $\mu_{Y|x_0}$ can now be constructed from the statistic
$$T = \frac{\hat{Y_0} - \mu_{Y|x_0}}{S \sqrt{[\frac{1}{n} + \frac{(x_0 - \bar{x})^2}{S_{xx}}]}}$$
which has a $t$-distribution with $n-2$ degrees of freedom.

To obtain a predicition interval for a single value $y_0$ of the variable $Y_0$ when $x = x_0$, it is better to consider the difference $\hat{y_0} - y_0$. The quantity $\hat{y_0} - y_0$ is a value taken by the random variable $\hat{Y_0} - Y_0$, whose sampling distribution can be shown to be normal with mean 
$$E(\hat{Y_0} - Y_0)=0$$
and variance
$$
\begin{aligned}
\sigma^2_{\hat{Y_0}} &= \sigma_{B_0 + B_1 x_0}^2 = \sigma^2_{\bar{Y} + B_1 (x_0 - \bar{x}) - \epsilon_0} \\
&= \sigma^2 \Big [ \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{S_{xx}}  \Big]
\end{aligned}
$$

Thus a $100(1-\alpha)\%$ confidence interval for a single predicted value $y_0$ can be constructed from the statistic
$$T = \frac{\hat{Y_0} - Y_0}{S \sqrt{[1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{S_{xx}}]}}$$
which has a $t$-distribution with $n-2$ degrees of freedom.


## Examples

#### No library done out in R

```{r}
df <- read.csv("TBSOpenDataVisits.csv", header = TRUE)

y_i <- df$downloads
x_i <- df$visits

plot(x = x_i, 
     y = y_i,
     main = "TBS Download Data",
     col="blue")
```
```{r}
# find beta0
```

```{r}
# find beta1
```
#### With R libraries

#### With python libraries





```{r}

```
